NAME:Oliver Goch
EMAIL:the.oliver.goch@gmail.com
ID:123456789

SortedList.h: Contains the functions and descriptions for the SortedList implementation.

SortedList.c: Contains the implementaiton for the SortedList Linked List.

lab2_list.c: This is the multithreaded linked list program. There are optional command line arguments for the number of threads, the number of iterations, whether to yield and where to yield, which calls sched_yield(), or synchronization options. The synchronization options are s for spin lock and m for mutex lock.

Makefile: A Makefile that has options for build that builds both programs, tests the 200 tests and generates CSV file, graphs that generates the necesary graphs, dist that makes the distribution tarball, and clean that removes all generated files.

.png files: Each one has a graph that was generated by the graphs command.

profile.out: An execution profiling report where the time was spent in the code.

README: The file that has a description of all other files, including this one! Also has answers to all the questions.

QUESTIONS

QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?

Why do you believe these to be the most expensive parts of the code?

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

ANSWER: For 1 and 2 thread lists with spin locks and mutex locks, there are four cases to examine. The first is 1 thread and spin lock: the most time is spent in list operations. In the case of 2 thread and spin locks, the time is divided half between spinning and list operations. In 1 thread with mutex lock, if the list is large the most time is spent on list operations, but if the list is small it is hard to tell. Similarly with 2 thread and mutex lock, if the list is large most time is spent on list operations and it is hard to tell if the list is small. The most expensive parts of the code are the locks. The most time spent in high thread spin locks is spinning, waiting for the lock to release. The high thread mutex tests spends most of the time in the list operations if the list is big, and it is hard to tell if the list is small, similar to before.

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

Why does this operation become so expensive with large numbers of threads?

ANSWER: The most cycles are spent waiting for the spin lock, so whenever the code is while(__sync_lock_test_and_set(&spinLock, 1) == 1), a lot of cycles are spent as this line. It becomes so expensive with a large number of threads because many threads are competing for the lock and then have to wait for it. As the number of threads grow, so do the queues for the lock and so do the number of cycles spent at the lock.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?


ANSWER: The average lock wait time rises so dramatically becasue the number of contending threads rises. With more threads competing for the lock there is more time wasted waiting for the lock to be released. Essentially the queue for the lock will be much longer. Completion time per operation rises with the number of contending threads because again of the number of threads. More contention means more of a queue, which means more locking on threads, which translates to longer operations. The wait time per operation goes up faster than the completion time because the length of the queus increases. More people waiting for the lock increases the wait time per operation.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

ANSWER: The change in performance is that the throughput increases with the number of lists. The throughput would not continue increasing as there is a cutoff point where the overhead gets too high and there is no benefit for having so many lists. It does appear that the throughput an N-way partitioned list would be equivalent to the throughput of a single list with fewer (1/N) threads. This would follow mathematically. It is viewable in my plots.

